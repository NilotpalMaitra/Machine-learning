{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNewYxkohm7aSGJlPIP4tai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nilvalox/AIML/blob/main/Grad_desc_for_log_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2yxvM26yna-"
      },
      "outputs": [],
      "source": [
        "import copy, math\n",
        "import numpy as np\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from lab_utils_common import  dlc, plot_data, plt_tumor_data, sigmoid, compute_cost_logistic\n",
        "from plt_quad_logistic import plt_quad_logistic, plt_prob\n",
        "plt.style.use('./deeplearning.mplstyle')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
        "y_train = np.array([0, 0, 0, 1, 1, 1])"
      ],
      "metadata": {
        "id": "m5VmgVpIytHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
        "plot_data(X_train, y_train, ax)\n",
        "\n",
        "ax.axis([0, 4, 0, 3.5])\n",
        "ax.set_ylabel('$x_1$', fontsize=12)\n",
        "ax.set_xlabel('$x_0$', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FQNhVZseyvFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Logistic Gradient Descent\n",
        "\n",
        "Recall the gradient descent algorithm utilizes the gradient calculation:\n",
        "repeat until convergence:{ğ‘¤ğ‘—=ğ‘¤ğ‘—âˆ’ğ›¼âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘¤ğ‘—ğ‘=ğ‘âˆ’ğ›¼âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘}for j := 0..n-1(1)\n",
        "Where each iteration performs simultaneous updates on  ğ‘¤ğ‘—\n",
        "  for all  ğ‘—\n",
        " , where\n",
        "âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘¤ğ‘—âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ‘¦(ğ‘–))ğ‘¥(ğ‘–)ğ‘—=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ‘¦(ğ‘–))(2)(3)\n",
        "m is the number of training examples in the data set\n",
        "ğ‘“ğ°,ğ‘(ğ‘¥(ğ‘–))\n",
        "  is the model's prediction, while  ğ‘¦(ğ‘–)\n",
        "  is the target\n",
        "For a logistic regression model\n",
        "ğ‘§=ğ°â‹…ğ±+ğ‘\n",
        "\n",
        "ğ‘“ğ°,ğ‘(ğ‘¥)=ğ‘”(ğ‘§)\n",
        "\n",
        "where  ğ‘”(ğ‘§)\n",
        "  is the sigmoid function:\n",
        "ğ‘”(ğ‘§)=11+ğ‘’âˆ’ğ‘§\n"
      ],
      "metadata": {
        "id": "iejT4az3y1Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_logistic(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for logistic regression\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "    Returns\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape\n",
        "    dj_dw = np.zeros((n,))                           #(n,)\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
        "        err_i  = f_wb_i  - y[i]                       #scalar\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
        "        dj_db = dj_db + err_i\n",
        "    dj_dw = dj_dw/m                                   #(n,)\n",
        "    dj_db = dj_db/m                                   #scalar\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "f4FlJHAgyyOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
        "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
        "w_tmp = np.array([2.,3.])\n",
        "b_tmp = 1.\n",
        "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
        "print(f\"dj_db: {dj_db_tmp}\" )\n",
        "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
      ],
      "metadata": {
        "id": "MnJi7_xTzAz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n)   : Data, m examples with n features\n",
        "      y (ndarray (m,))   : target values\n",
        "      w_in (ndarray (n,)): Initial values of model parameters\n",
        "      b_in (scalar)      : Initial values of model parameter\n",
        "      alpha (float)      : Learning rate\n",
        "      num_iters (scalar) : number of iterations to run gradient descent\n",
        "\n",
        "    Returns:\n",
        "      w (ndarray (n,))   : Updated values of parameters\n",
        "      b (scalar)         : Updated value of parameter\n",
        "    \"\"\"\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)\n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw\n",
        "        b = b - alpha * dj_db\n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion\n",
        "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
        "\n",
        "    return w, b, J_history         #return final w,b and J history for graphing"
      ],
      "metadata": {
        "id": "PijgyAPfzDOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_tmp  = np.zeros_like(X_train[0])\n",
        "b_tmp  = 0.\n",
        "alph = 0.1\n",
        "iters = 10000\n",
        "\n",
        "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters)\n",
        "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
      ],
      "metadata": {
        "id": "AONklpFtzF6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(5,4))\n",
        "# plot the probability\n",
        "plt_prob(ax, w_out, b_out)\n",
        "\n",
        "# Plot the original data\n",
        "ax.set_ylabel(r'$x_1$')\n",
        "ax.set_xlabel(r'$x_0$')\n",
        "ax.axis([0, 4, 0, 3.5])\n",
        "plot_data(X_train,y_train,ax)\n",
        "\n",
        "# Plot the decision boundary\n",
        "x0 = -b_out/w_out[0]\n",
        "x1 = -b_out/w_out[1]\n",
        "ax.plot([0,x0],[x1,0], c=dlc[\"dlblue\"], lw=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oU433ebRzJEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([0., 1, 2, 3, 4, 5])\n",
        "y_train = np.array([0,  0, 0, 1, 1, 1])"
      ],
      "metadata": {
        "id": "yGMUa2eYzLRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
        "plt_tumor_data(x_train, y_train, ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "utZ6aL4czNGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_range = np.array([-1, 7])\n",
        "b_range = np.array([1, -14])\n",
        "quad = plt_quad_logistic( x_train, y_train, w_range, b_range )"
      ],
      "metadata": {
        "id": "jD4vTxJkzPWK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}